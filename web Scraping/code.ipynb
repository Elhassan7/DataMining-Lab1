{"cells":[{"cell_type":"markdown","metadata":{},"source":["In this notebook, me and my team, we are going to scrape an ecommerce website. We'll get into each individual product page and retrieve our information from there. This is the website we are going to scrape <a href=\"https://www.amazon.com/s?i=mobile&rh=n%3A2335752011%2Cp_72%3A2491149011&pd_rd_r=30305d08-93c6-4f70-aca0-5bdcd0619f0a&pd_rd_w=V3rlu&pd_rd_wg=cnq1i&pf_rd_p=f5c158e1-98f7-4998-94b8-d7306c066086&pf_rd_r=Q0RJ0DQSNB5EM8EWZX6G&qid=1649783074&ref=sr_pg_1\">Amazon</a>  – it's an French online shop."]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","import ast\n"]},{"cell_type":"markdown","metadata":{},"source":["Maintenant, nous allons définir l'URL de base de la page principale car nous en aurons besoin lorsque nous construirons nos URL pour chacun des produits individuels.\n","\n","De plus, nous enverrons un agent utilisateur sur chaque requête HTTP, car si nous avons effectuez une requête GET à l'aide de requests , l'agent utilisateur est Python par défaut, ce qui peut être bloqué.\n","\n","Donc, pour remplacer cela, nous allons déclarer une variable qui stockera notre user-agent."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["from selectorlib import Extractor\n","import requests \n","import json \n","from time import sleep"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1158\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from ast import literal_eval\n","\n","data=open(\"D:\\INPT2\\Data mining\\DataMining-Project\\web Scraping\\output_1.json\",\"r\")\n","data=[literal_eval(i) for i in data]\n","print(len(data))\n","df = pd.DataFrame.from_dict(data, orient='columns')\n","df=df.replace(\"null\", np.NaN)"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/plain":["['name',\n"," 'price',\n"," 'images',\n"," 'rating',\n"," 'product_description',\n"," 'number_of_reviews']"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["list(df.columns)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["name                   926\n","price                  977\n","images                 926\n","rating                 946\n","product_description    936\n","number_of_reviews      946\n","dtype: int64"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["df.isnull().sum()"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["df=df.dropna(axis=0, how=\"all\")\n","df=df.drop_duplicates()"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 107 entries, 13 to 1151\n","Data columns (total 6 columns):\n"," #   Column               Non-Null Count  Dtype \n","---  ------               --------------  ----- \n"," 0   name                 107 non-null    object\n"," 1   price                57 non-null     object\n"," 2   images               107 non-null    object\n"," 3   rating               95 non-null     object\n"," 4   product_description  101 non-null    object\n"," 5   number_of_reviews    95 non-null     object\n","dtypes: object(6)\n","memory usage: 5.9+ KB\n"]}],"source":["df.info()"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["df.to_csv(\"D:\\INPT2\\Data mining\\DataMining-Project\\web Scraping\\Data_Amazon.csv\", mode='a', header=False)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["df=pd.read_csv(\"D:\\INPT2\\Data mining\\DataMining-Project\\web Scraping\\Data_Amazon.csv\")\n"]},{"cell_type":"markdown","metadata":{},"source":["### transform product's images link to list"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[],"source":["df[\"images\"]=df[\"images\"].apply(lambda x: list(ast.literal_eval(x).keys()))"]},{"cell_type":"markdown","metadata":{},"source":["### Cleaning Name and Product_Description"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[],"source":["from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","import string\n","\n","from nltk.stem.porter import PorterStemmer"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["def cleaning(text):\n","    # split into words\n","    tokens = word_tokenize(text)\n","    # convert to lower case\n","    tokens = [w.lower() for w in tokens]\n","    # remove punctuation from each word\n","    table = str.maketrans('', '', string.punctuation)\n","    stripped = [w.translate(table) for w in tokens]\n","    # remove remaining tokens that are not alphabetic\n","    words = [word for word in stripped if word.isalpha()]\n","    # filter out stop words\n","    stop_words = set(stopwords.words('english'))\n","    words = [w for w in words if not w in stop_words]\n","    #Stemming Words\n","    porter = PorterStemmer()\n","    stemmed = [porter.stem(word) for word in words]\n","\n","    return stemmed"]},{"cell_type":"markdown","metadata":{},"source":["apply function cleaning in name and product_description"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[{"ename":"TypeError","evalue":"expected string or bytes-like object","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32md:\\INPT2\\Data mining\\DataMining-Project\\web Scraping\\code.ipynb Cell 18'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/INPT2/Data%20mining/DataMining-Project/web%20Scraping/code.ipynb#ch0000025?line=0'>1</a>\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39mname_clean\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m=\u001b[39mdf[\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: cleaning(x))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/INPT2/Data%20mining/DataMining-Project/web%20Scraping/code.ipynb#ch0000025?line=1'>2</a>\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39mcategory_clean\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m=\u001b[39mdf[\u001b[39m\"\u001b[39;49m\u001b[39mproduct_description\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x: cleaning(x))\n","File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\series.py:4430\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/series.py?line=4319'>4320</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/series.py?line=4320'>4321</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/series.py?line=4321'>4322</a>\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/series.py?line=4324'>4325</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/series.py?line=4325'>4326</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/series.py?line=4326'>4327</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/series.py?line=4327'>4328</a>\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/series.py?line=4328'>4329</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/series.py?line=4427'>4428</a>\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/series.py?line=4428'>4429</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/series.py?line=4429'>4430</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n","File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:1082\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/apply.py?line=1077'>1078</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mstr\u001b[39m):\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/apply.py?line=1078'>1079</a>\u001b[0m     \u001b[39m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/apply.py?line=1079'>1080</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[1;32m-> <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/apply.py?line=1081'>1082</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n","File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:1137\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/apply.py?line=1130'>1131</a>\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/apply.py?line=1131'>1132</a>\u001b[0m         \u001b[39m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/apply.py?line=1132'>1133</a>\u001b[0m         \u001b[39m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/apply.py?line=1133'>1134</a>\u001b[0m         \u001b[39m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/apply.py?line=1134'>1135</a>\u001b[0m         \u001b[39m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/apply.py?line=1135'>1136</a>\u001b[0m         \u001b[39m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/apply.py?line=1136'>1137</a>\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/apply.py?line=1137'>1138</a>\u001b[0m             values,\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/apply.py?line=1138'>1139</a>\u001b[0m             f,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/apply.py?line=1139'>1140</a>\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/apply.py?line=1140'>1141</a>\u001b[0m         )\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/apply.py?line=1142'>1143</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/apply.py?line=1143'>1144</a>\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/apply.py?line=1144'>1145</a>\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/apply.py?line=1145'>1146</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n","File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n","\u001b[1;32md:\\INPT2\\Data mining\\DataMining-Project\\web Scraping\\code.ipynb Cell 18'\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/INPT2/Data%20mining/DataMining-Project/web%20Scraping/code.ipynb#ch0000025?line=0'>1</a>\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39mname_clean\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m=\u001b[39mdf[\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: cleaning(x))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/INPT2/Data%20mining/DataMining-Project/web%20Scraping/code.ipynb#ch0000025?line=1'>2</a>\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39mcategory_clean\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m=\u001b[39mdf[\u001b[39m\"\u001b[39m\u001b[39mproduct_description\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: cleaning(x))\n","\u001b[1;32md:\\INPT2\\Data mining\\DataMining-Project\\web Scraping\\code.ipynb Cell 16'\u001b[0m in \u001b[0;36mcleaning\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/INPT2/Data%20mining/DataMining-Project/web%20Scraping/code.ipynb#ch0000023?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcleaning\u001b[39m(text):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/INPT2/Data%20mining/DataMining-Project/web%20Scraping/code.ipynb#ch0000023?line=1'>2</a>\u001b[0m     \u001b[39m# split into words\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/INPT2/Data%20mining/DataMining-Project/web%20Scraping/code.ipynb#ch0000023?line=2'>3</a>\u001b[0m     tokens \u001b[39m=\u001b[39m word_tokenize(text)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/INPT2/Data%20mining/DataMining-Project/web%20Scraping/code.ipynb#ch0000023?line=3'>4</a>\u001b[0m     \u001b[39m# convert to lower case\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/INPT2/Data%20mining/DataMining-Project/web%20Scraping/code.ipynb#ch0000023?line=4'>5</a>\u001b[0m     tokens \u001b[39m=\u001b[39m [w\u001b[39m.\u001b[39mlower() \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m tokens]\n","File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/__init__.py?line=113'>114</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/__init__.py?line=114'>115</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/__init__.py?line=115'>116</a>\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/__init__.py?line=116'>117</a>\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/__init__.py?line=126'>127</a>\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/__init__.py?line=127'>128</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/__init__.py?line=128'>129</a>\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/__init__.py?line=129'>130</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m    <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/__init__.py?line=130'>131</a>\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[0;32m    <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/__init__.py?line=131'>132</a>\u001b[0m     ]\n","File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\__init__.py:107\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/__init__.py?line=96'>97</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/__init__.py?line=97'>98</a>\u001b[0m \u001b[39mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/__init__.py?line=98'>99</a>\u001b[0m \u001b[39musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/__init__.py?line=103'>104</a>\u001b[0m \u001b[39m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/__init__.py?line=104'>105</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/__init__.py?line=105'>106</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m load(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtokenizers/punkt/\u001b[39m\u001b[39m{\u001b[39;00mlanguage\u001b[39m}\u001b[39;00m\u001b[39m.pickle\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/__init__.py?line=106'>107</a>\u001b[0m \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39;49mtokenize(text)\n","File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1276\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=1271'>1272</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize\u001b[39m(\u001b[39mself\u001b[39m, text, realign_boundaries\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=1272'>1273</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=1273'>1274</a>\u001b[0m \u001b[39m    Given a text, returns a list of the sentences in that text.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=1274'>1275</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=1275'>1276</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msentences_from_text(text, realign_boundaries))\n","File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1332\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.sentences_from_text\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=1324'>1325</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msentences_from_text\u001b[39m(\u001b[39mself\u001b[39m, text, realign_boundaries\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=1325'>1326</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=1326'>1327</a>\u001b[0m \u001b[39m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=1327'>1328</a>\u001b[0m \u001b[39m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=1328'>1329</a>\u001b[0m \u001b[39m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=1329'>1330</a>\u001b[0m \u001b[39m    follows the period.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=1330'>1331</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=1331'>1332</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m [text[s:e] \u001b[39mfor\u001b[39;00m s, e \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n","File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1332\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=1324'>1325</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msentences_from_text\u001b[39m(\u001b[39mself\u001b[39m, text, realign_boundaries\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=1325'>1326</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=1326'>1327</a>\u001b[0m \u001b[39m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=1327'>1328</a>\u001b[0m \u001b[39m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=1328'>1329</a>\u001b[0m \u001b[39m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=1329'>1330</a>\u001b[0m \u001b[39m    follows the period.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=1330'>1331</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=1331'>1332</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m [text[s:e] \u001b[39mfor\u001b[39;00m s, e \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n","File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1322\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.span_tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=1319'>1320</a>\u001b[0m \u001b[39mif\u001b[39;00m realign_boundaries:\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=1320'>1321</a>\u001b[0m     slices \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_realign_boundaries(text, slices)\n\u001b[1;32m-> <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=1321'>1322</a>\u001b[0m \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m slices:\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=1322'>1323</a>\u001b[0m     \u001b[39myield\u001b[39;00m (sentence\u001b[39m.\u001b[39mstart, sentence\u001b[39m.\u001b[39mstop)\n","File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1421\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._realign_boundaries\u001b[1;34m(self, text, slices)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=1407'>1408</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=1408'>1409</a>\u001b[0m \u001b[39mAttempts to realign punctuation that falls after the period but\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=1409'>1410</a>\u001b[0m \u001b[39mshould otherwise be included in the same sentence.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=1417'>1418</a>\u001b[0m \u001b[39m    [\"(Sent1.)\", \"Sent2.\"].\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=1418'>1419</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=1419'>1420</a>\u001b[0m realign \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m-> <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=1420'>1421</a>\u001b[0m \u001b[39mfor\u001b[39;00m sentence1, sentence2 \u001b[39min\u001b[39;00m _pair_iter(slices):\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=1421'>1422</a>\u001b[0m     sentence1 \u001b[39m=\u001b[39m \u001b[39mslice\u001b[39m(sentence1\u001b[39m.\u001b[39mstart \u001b[39m+\u001b[39m realign, sentence1\u001b[39m.\u001b[39mstop)\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=1422'>1423</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m sentence2:\n","File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\punkt.py:318\u001b[0m, in \u001b[0;36m_pair_iter\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=315'>316</a>\u001b[0m iterator \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(iterator)\n\u001b[0;32m    <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=316'>317</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=317'>318</a>\u001b[0m     prev \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(iterator)\n\u001b[0;32m    <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=318'>319</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=319'>320</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m\n","File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1395\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._slices_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=1392'>1393</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_slices_from_text\u001b[39m(\u001b[39mself\u001b[39m, text):\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=1393'>1394</a>\u001b[0m     last_break \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m-> <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=1394'>1395</a>\u001b[0m     \u001b[39mfor\u001b[39;00m match, context \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_match_potential_end_contexts(text):\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=1395'>1396</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_contains_sentbreak(context):\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=1396'>1397</a>\u001b[0m             \u001b[39myield\u001b[39;00m \u001b[39mslice\u001b[39m(last_break, match\u001b[39m.\u001b[39mend())\n","File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1375\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._match_potential_end_contexts\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=1372'>1373</a>\u001b[0m before_words \u001b[39m=\u001b[39m {}\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=1373'>1374</a>\u001b[0m matches \u001b[39m=\u001b[39m []\n\u001b[1;32m-> <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=1374'>1375</a>\u001b[0m \u001b[39mfor\u001b[39;00m match \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(\u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_lang_vars\u001b[39m.\u001b[39;49mperiod_context_re()\u001b[39m.\u001b[39;49mfinditer(text))):\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=1375'>1376</a>\u001b[0m     \u001b[39m# Ignore matches that have already been captured by matches to the right of this match\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=1376'>1377</a>\u001b[0m     \u001b[39mif\u001b[39;00m matches \u001b[39mand\u001b[39;00m match\u001b[39m.\u001b[39mend() \u001b[39m>\u001b[39m before_start:\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python310/lib/site-packages/nltk/tokenize/punkt.py?line=1377'>1378</a>\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n","\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"]}],"source":["df[\"name_clean\"]=df[\"name\"].apply(lambda x: cleaning(x))\n","df[\"category_clean\"]=df[\"product_description\"].apply(lambda x: cleaning(x))"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>name</th>\n","      <th>name_clean</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Dirt Devil Simpli-Stik Vacuum Cleaner, 3-in-1 ...</td>\n","      <td>[dirt, devil, simplistik, vacuum, cleaner, han...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Tineco Floor ONE S3 Cordless Hardwood Floors C...</td>\n","      <td>[tineco, floor, one, cordless, hardwood, floor...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Drop PLNT HNGR 4FT 3/8\"12</td>\n","      <td>[drop, plnt, hngr]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Cuisinart Waffle Maker, Double Belgian, Stainl...</td>\n","      <td>[cuisinart, waffl, maker, doubl, belgian, stai...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Waterdrop DA29-00020B Refrigerator Water Filte...</td>\n","      <td>[waterdrop, refriger, water, filter, replac, s...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                name  \\\n","0  Dirt Devil Simpli-Stik Vacuum Cleaner, 3-in-1 ...   \n","1  Tineco Floor ONE S3 Cordless Hardwood Floors C...   \n","2                          Drop PLNT HNGR 4FT 3/8\"12   \n","3  Cuisinart Waffle Maker, Double Belgian, Stainl...   \n","4  Waterdrop DA29-00020B Refrigerator Water Filte...   \n","\n","                                          name_clean  \n","0  [dirt, devil, simplistik, vacuum, cleaner, han...  \n","1  [tineco, floor, one, cordless, hardwood, floor...  \n","2                                 [drop, plnt, hngr]  \n","3  [cuisinart, waffl, maker, doubl, belgian, stai...  \n","4  [waterdrop, refriger, water, filter, replac, s...  "]},"execution_count":60,"metadata":{},"output_type":"execute_result"}],"source":["df[[\"name\",\"name_clean\"]].head(5)"]},{"cell_type":"markdown","metadata":{},"source":["# -----------------"]}],"metadata":{"interpreter":{"hash":"5d240ba0dc525c389faa33f5dcce5b4f32b6d6aa6d70d6d2dd929bd2b09ab69f"},"kernelspec":{"display_name":"Python 3.10.2 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
